{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YohPLlZKEqRN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import shutil\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfb0VoYqEtO0",
        "outputId": "3172972c-31a3-4aab-f43d-a9062801de29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KclLBLVODaqe"
      },
      "outputs": [],
      "source": [
        "# # ==============================\n",
        "# # Konfigurasi Path dan Proporsi BARU\n",
        "# # ==============================\n",
        "# # Path ke dataset asli (folder yang berisi 30 kelas)\n",
        "# BASE_DIR = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/Food-image-classification/Fruit-And-Vegetable-Diseases-Dataset'\n",
        "\n",
        "# # Path untuk direktori baru (Hanya 2 kelas: Healthy, Rotten)\n",
        "# # Dibuat di luar Split_Dataset lama\n",
        "# NEW_BASE_DIR = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/Binary_Split_Dataset'\n",
        "\n",
        "# # Proporsi pembagian (tetap sama)\n",
        "# TRAIN_SPLIT = 0.8\n",
        "# VAL_SPLIT = 0.1\n",
        "# TEST_SPLIT = 0.1\n",
        "\n",
        "# # ==============================\n",
        "# # Fungsi Utama Binary Splitting\n",
        "# # ==============================\n",
        "# def split_data_binary(base_source_dir, base_target_dir, split_ratio):\n",
        "#     \"\"\"\n",
        "#     Menggabungkan semua gambar dari 30 sub-folder menjadi 2 kelas (Healthy/Rotten)\n",
        "#     dan membaginya menjadi Train, Val, Test.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Inisialisasi folder 2 kelas di Train/Val/Test\n",
        "#     for set_name in ['train', 'validation', 'test']:\n",
        "#         os.makedirs(os.path.join(base_target_dir, set_name, 'Healthy'), exist_ok=True)\n",
        "#         os.makedirs(os.path.join(base_target_dir, set_name, 'Rotten'), exist_ok=True)\n",
        "\n",
        "#     # Dapatkan daftar semua folder kelas lama (misal: Apple_Healthy, Tomato_Rotten)\n",
        "#     old_class_names = [d for d in os.listdir(base_source_dir)\n",
        "#                        if os.path.isdir(os.path.join(base_source_dir, d))]\n",
        "\n",
        "#     if not old_class_names:\n",
        "#         print(\"\\nERROR: Tidak ada sub-direktori kelas yang ditemukan di BASE_DIR.\")\n",
        "#         return\n",
        "\n",
        "#     # Dictionary untuk mengumpulkan semua file berdasarkan kondisi (Healthy/Rotten)\n",
        "#     all_files_by_condition = {'Healthy': [], 'Rotten': []}\n",
        "\n",
        "#     for class_name in old_class_names:\n",
        "#         source_path = os.path.join(base_source_dir, class_name)\n",
        "\n",
        "#         # Tentukan kondisi baru: Healthy atau Rotten\n",
        "#         if class_name.endswith('_Healthy'):\n",
        "#             condition = 'Healthy'\n",
        "#         elif class_name.endswith('_Rotten'):\n",
        "#             condition = 'Rotten'\n",
        "#         else:\n",
        "#             print(f\"Mengabaikan folder: {class_name}\")\n",
        "#             continue\n",
        "\n",
        "#         # Kumpulkan semua file di folder ini\n",
        "#         current_files = [(filename, class_name) for filename in os.listdir(source_path)\n",
        "#                          if os.path.isfile(os.path.join(source_path, filename))]\n",
        "\n",
        "#         all_files_by_condition[condition].extend(current_files)\n",
        "#         print(f\"Mengumpulkan {len(current_files)} gambar dari {class_name} ke {condition}.\")\n",
        "\n",
        "\n",
        "#     # ==================================\n",
        "#     # 2. Pembagian dan Penyalinan Data\n",
        "#     # ==================================\n",
        "#     for condition, file_list in all_files_by_condition.items():\n",
        "#         random.shuffle(file_list) # Acak semua file\n",
        "\n",
        "#         # Hitung pembagian\n",
        "#         total_count = len(file_list)\n",
        "#         train_count = int(total_count * split_ratio['train'])\n",
        "#         val_count = int(total_count * split_ratio['val'])\n",
        "\n",
        "#         train_files = file_list[:train_count]\n",
        "#         val_files = file_list[train_count:train_count + val_count]\n",
        "#         test_files = file_list[train_count + val_count:]\n",
        "\n",
        "#         print(f\"\\nKondisi: {condition} (Total: {total_count} gambar)\")\n",
        "#         print(f\"  Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
        "\n",
        "#         # Salin file\n",
        "#         for files, set_name in zip([train_files, val_files, test_files], ['train', 'validation', 'test']):\n",
        "#             target_dir = os.path.join(base_target_dir, set_name, condition)\n",
        "#             for filename, old_folder in files:\n",
        "#                 source_file = os.path.join(base_source_dir, old_folder, filename)\n",
        "#                 target_file = os.path.join(target_dir, f\"{old_folder}_{filename}\") # Beri nama unik\n",
        "#                 shutil.copy(source_file, target_file)\n",
        "\n",
        "\n",
        "# # ==============================\n",
        "# # Jalankan Skrip Utama\n",
        "# # ==============================\n",
        "# if __name__ == '__main__':\n",
        "#     # --- LOGIKA PENGHAPUSAN OTOMATIS ---\n",
        "#     if os.path.exists(NEW_BASE_DIR):\n",
        "#         print(f\"Direktori lama '{NEW_BASE_DIR}' ditemukan. Menghapus direktori lama...\")\n",
        "#         shutil.rmtree(NEW_BASE_DIR)\n",
        "#         print(\"Direktori lama berhasil dihapus.\")\n",
        "\n",
        "#     print(f\"\\nMembuat direktori baru untuk Binary Classification: {NEW_BASE_DIR}\")\n",
        "#     os.makedirs(NEW_BASE_DIR)\n",
        "\n",
        "#     split_data_binary(BASE_DIR, NEW_BASE_DIR,\n",
        "#                       split_ratio={'train': TRAIN_SPLIT, 'val': VAL_SPLIT, 'test': TEST_SPLIT})\n",
        "\n",
        "#     print(\"\\n=======================================================\")\n",
        "#     print(\"Proses pembagian dataset BINARY (Healthy/Rotten) selesai!\")\n",
        "#     print(f\"Dataset baru Anda berada di: {NEW_BASE_DIR}\")\n",
        "#     print(\"=======================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Z2gLcHGpvu",
        "outputId": "a7d145e8-77a4-4c88-bf3e-01833a118a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 23421 images belonging to 2 classes.\n",
            "Found 2926 images belonging to 2 classes.\n",
            "Found 2930 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "IMAGE_SIZE = (150, 150)\n",
        "BATCH_SIZE = 32\n",
        "# NUM_CLASSES = 4 (Diabaikan di sini, tetapi di model akan menjadi 1)\n",
        "\n",
        "# Path baru setelah binary split\n",
        "train_dir_binary = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/Binary_Split_Dataset/train'\n",
        "validation_dir_binary = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/Binary_Split_Dataset/validation'\n",
        "test_dir_binary = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/Binary_Split_Dataset/test'\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# 1. Image Data Generator untuk Training (Augmentasi Aggressive)\n",
        "# =======================================================\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=[0.8, 1.2],\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir_binary, # PATH BARU\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', # KUNCI: Diubah menjadi 'binary'\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# =======================================================\n",
        "# 2. Image Data Generator untuk Validation dan Test (Hanya Normalisasi)\n",
        "# =======================================================\n",
        "validation_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_test_datagen.flow_from_directory(\n",
        "    validation_dir_binary, # PATH BARU\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', # KUNCI: Diubah menjadi 'binary'\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = validation_test_datagen.flow_from_directory(\n",
        "    test_dir_binary, # PATH BARU\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', # KUNCI: Diubah menjadi 'binary'\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "ra5Mo0DVHK-W",
        "outputId": "7981d0a5-ece8-45b6-9815-bfbffe9dc888"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41472</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41472\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,308,544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,403,329</span> (20.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,403,329\u001b[0m (20.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,402,625</span> (20.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,402,625\u001b[0m (20.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memulai pelatihan model (Binary Classification: Sederhana)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m133/731\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17:56\u001b[0m 14s/step - accuracy: 0.5818 - loss: 1.1885"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m731/731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - accuracy: 0.6513 - loss: 0.9897 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.68613, saving model to /content/drive/MyDrive/Deep-Learning/Food-Image-Classification/best_model_checkpoint/best_fruit_disease_cnn_binary_simple.keras\n",
            "\u001b[1m731/731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12075s\u001b[0m 17s/step - accuracy: 0.6513 - loss: 0.9895 - val_accuracy: 0.6861 - val_loss: 0.7483 - learning_rate: 5.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m  1/731\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51:51\u001b[0m 4s/step - accuracy: 0.6875 - loss: 0.8815"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.68613\n",
            "\u001b[1m731/731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 111ms/step - accuracy: 0.6875 - loss: 0.8815 - val_accuracy: 0.6703 - val_loss: 0.7864 - learning_rate: 5.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m128/731\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:30\u001b[0m 3s/step - accuracy: 0.7617 - loss: 0.6918"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# --- 1. DEFINISI PARAMETER DAN MODEL (Sederhana) ---\n",
        "IMAGE_SIZE = (150, 150)\n",
        "NUM_CLASSES = 1 # Wajib untuk biner\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Lapisan 1: Blok Fitur Dasar (Filters dikurangi sedikit)\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Lapisan 2: Blok Fitur Menengah\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Dropout(0.2)) # Mengurangi Dropout\n",
        "\n",
        "# Lapisan 3: Blok Fitur Kompleks (Cukup)\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "# Lapisan 4 Dihapus!\n",
        "\n",
        "# Lapisan Fully Connected\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001))) # Mengurangi unit Dense dari 256 menjadi 128\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5)) # Mengurangi Dropout dari 0.7 menjadi 0.5\n",
        "\n",
        "# Lapisan Output (Wajib: 1 unit dan Sigmoid)\n",
        "model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "# --- 2. KOMPILASI MODEL ---\n",
        "# Learning Rate bisa sedikit dinaikkan karena tugasnya lebih mudah\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Ringkasan model yang disederhanakan\n",
        "model.summary()\n",
        "\n",
        "# --- 3. CALLBACKS (Dapat dipertahankan, karena sudah efektif) ---\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Deep-Learning/Food-Image-Classification/best_model_checkpoint/best_fruit_disease_cnn_binary_simple.keras'\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks_list = [model_checkpoint_callback, early_stopping_callback, lr_reducer]\n",
        "\n",
        "# --- 4. PELATIHAN MODEL ---\n",
        "# ASUMSI: train_generator dan validation_generator sudah disesuaikan\n",
        "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "STEP_SIZE_VALIDATION = validation_generator.n // validation_generator.batch_size\n",
        "EPOCHS = 15\n",
        "\n",
        "print(\"Memulai pelatihan model (Binary Classification: Sederhana)...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=STEP_SIZE_VALIDATION,\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "print(\"\\nPelatihan selesai! Model terbaik telah disimpan.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3X9hACiHekz"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoPzwwsIHiUh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ambil data akurasi dan loss dari objek history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Tentukan jumlah epoch\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# ------------------------------------\n",
        "# Plot Akurasi Training dan Validation\n",
        "# ------------------------------------\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc, 'r', label='Akurasi Training')\n",
        "plt.plot(epochs, val_acc, 'b', label='Akurasi Validation')\n",
        "plt.title('Akurasi Training dan Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "# ------------------------------------\n",
        "# Plot Loss Training dan Validation\n",
        "# ------------------------------------\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'r', label='Loss Training')\n",
        "plt.plot(epochs, val_loss, 'b', label='Loss Validation')\n",
        "plt.title('Loss Training dan Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua6Gh_B7HkEB"
      },
      "outputs": [],
      "source": [
        "# Tentukan langkah untuk evaluasi (Total Test Images / Batch Size)\n",
        "STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
        "\n",
        "print(\"Mengevaluasi model pada data test...\")\n",
        "\n",
        "# Melakukan evaluasi model\n",
        "loss_test, acc_test = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=STEP_SIZE_TEST,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n=======================================================\")\n",
        "print(f\"Hasil Evaluasi Akhir pada Test Set (Kriteria 5):\")\n",
        "print(f\"Loss Test: {loss_test:.4f}\")\n",
        "print(f\"Akurasi Test: {acc_test*100:.2f}%\")\n",
        "print(\"=======================================================\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}